{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Default Prediction: A Neural Network Approach\n",
    "22 July 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Matplotlib\n",
    "plt.rcParams['figure.figsize'] = 16,6\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Seaborn\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette(\"PuBuGn_d\")\n",
    "\n",
    "# Keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Sci-Kit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The data that we will be using comes from the Lending Club Company -- a peer-to-peer lending company. The data contains various features describing the loans issued through the said company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Data Directory\n",
    "data_dir = '../Data_Hands-on/'\n",
    "\n",
    "# Read Files\n",
    "X_train = pd.read_csv(data_dir + 'X_train.csv')\n",
    "X_test  = pd.read_csv(data_dir + 'X_test.csv' )\n",
    "y_train = pd.read_csv(data_dir + 'y_train.csv')\n",
    "y_test  = pd.read_csv(data_dir + 'y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing\n",
    "Fill all NaNs with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization: It is always a good practice to standardize the numerical features so that any numerical computations during training will be executed in a fast manner since we will be able to avoid operations involving significantly large values.\n",
    "\n",
    "Note that the proper way to standardize the test set is to use the parameters ($\\mu$ and $\\sigma$) from the training set. This is to ensure that any predictions on unseen data will still be scored relative to how the model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler class\n",
    "stnd_scl = StandardScaler()\n",
    "\n",
    "# Fit and transform training set\n",
    "X_train_stnd = stnd_scl.fit_transform(X_train)\n",
    "\n",
    "# Transform test set\n",
    "X_test_stnd = stnd_scl.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "For simplicity, we would just create a \"Vanilla\" Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the outline of the model architecture that we intend to build:\n",
    "- 1 input layer \n",
    "- 1 densely connected (hidden) layer\n",
    "- 1 dropout layer to minimize the effect of overfitting\n",
    "- 1 output layer (densely connected layer with 1 output node)\n",
    "\n",
    "<i>Rule of Thumb: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#:~:text=The%20number%20of%20hidden%20neurons,size%20of%20the%20input%20layer. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../images/nn.svg\" width=\"4000\" height=\"4000\"/> </center>\n",
    "\n",
    "<center> <i>The network architecture for this sample notebook </i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Use sequential model since the order of the layers is important\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add the input layer\n",
    "model.add(keras.layers.Dense(\n",
    "            units  = 11,                   # Number of features that will be passed on to the next layer\n",
    "            input_dim = X_train.shape[1],  # Input demnsions = number of features in training set\n",
    "            activation = 'relu'            # Use ReLU to avoid the problem of vanishing gradient\n",
    "            )\n",
    "         )\n",
    "\n",
    "# Add dropout layer to minimize overfitting\n",
    "model.add(keras.layers.Dropout(\n",
    "             rate=0.5 \n",
    "            )\n",
    "         )\n",
    "\n",
    "# Add output layer\n",
    "model.add(keras.layers.Dense(\n",
    "            units = 1,                     # One output node to signify the probability of good/bad loan\n",
    "            activation = 'sigmoid'         # 'Sigmoid' is used to ensure that the output is between 0 and 1\n",
    "            )\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(50000), Dimension(1)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_output_shape(input_shape = (50000,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be dealing with binary classifications (probability values within 0 and 1), our loss function should be <b>Binary Crossentropy</b>. For now we will use the <b>Adam</b> optimizer since it is the most flexible optimzer available in Keras (best optimizer to deal with non convex problems).\n",
    "\n",
    "$$\n",
    "L_{\\text{Cross_Entropy}} = -\\left[y \\ln(p) + (1-y) \\ln(1-p)\\right]\n",
    "$$\n",
    "\n",
    "where $y$ can only be $0$ or $1$, and $p$ is predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer ='adam',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on Epoch and Batch\n",
    "- <b> Epoch </b> - One pass through all the rows in the training dataset\n",
    "- <b> Batch </b> - One or more samples considered by the model within an epoch before weights are updated\n",
    "\n",
    "<i> machinelearningmastery.com/tutorial-first-neural-network-python-keras/ </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all Neural Network Models, the training process will run for a number of iterations through the entire dataset (this is called the epoch). We do not update the weights of the model for each data/row, but are updated based on the loss of an entire batch (average). Note that the lower the batch size and the higher the epoch values, the longer the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to determine the performance of our model to unseen data. We then need to split our training set further to accommodate the validation set. Let us choose 45,000  random rows to be the ones to update the weights, while the remaining 5,000  will be used to compute the validation accuracy of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(69) # Reproducibility\n",
    "\n",
    "# Shuffle the response variable together with the features\n",
    "val = np.hstack([X_train_stnd, y_train])\n",
    "np.random.shuffle(val)\n",
    "\n",
    "# Split Training Set\n",
    "X_train_stnd_not_val = val[:45000, :-1]\n",
    "X_train_stnd_val     = val[45000:, :-1]\n",
    "y_train_not_val      = val[:45000, -1]\n",
    "y_train_val           = val[45000:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training history to see if there is overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 20s 437us/sample - loss: 0.3254 - acc: 0.8485 - val_loss: 0.0834 - val_acc: 0.9798\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 20s 446us/sample - loss: 0.1096 - acc: 0.9673 - val_loss: 0.0494 - val_acc: 0.9862\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 20s 446us/sample - loss: 0.0882 - acc: 0.9742 - val_loss: 0.0439 - val_acc: 0.9880\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 20s 436us/sample - loss: 0.0766 - acc: 0.9756 - val_loss: 0.0388 - val_acc: 0.9880\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 20s 440us/sample - loss: 0.0727 - acc: 0.9775 - val_loss: 0.0354 - val_acc: 0.9886\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 20s 436us/sample - loss: 0.0673 - acc: 0.9784 - val_loss: 0.0331 - val_acc: 0.9898\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 21s 458us/sample - loss: 0.0643 - acc: 0.9792 - val_loss: 0.0336 - val_acc: 0.9894\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 21s 465us/sample - loss: 0.0634 - acc: 0.9798 - val_loss: 0.0323 - val_acc: 0.9900\n",
      "Epoch 9/10\n",
      "21456/45000 [=============>................] - ETA: 10s - loss: 0.0623 - acc: 0.9793"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_stnd_not_val, y_train_not_val, epochs = 10,\n",
    "                   validation_data=(X_train_stnd_val,y_train_val), batch_size = 16\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(hist, v1 = False):\n",
    "    \"\"\"Creates two plots. One is the loss vs epoch graph, and the other one\n",
    "    is the accuracy vs epoch graph\n",
    "    \"\"\"\n",
    "    if v1:\n",
    "        acc = 'acc'\n",
    "        val_acc = 'val_acc'\n",
    "    else:\n",
    "        acc = 'accuracy'\n",
    "        val_acc = 'val_accuracy'\n",
    "    # Obtain range of x values\n",
    "    x = range(len(hist['loss']))\n",
    "    \n",
    "    # Create dataframes\n",
    "    for_plot_df_loss = pd.DataFrame({'Epoch':x,\n",
    "                               'Loss':hist['loss'],\n",
    "                                'Validation Loss':hist['val_loss']})\n",
    "    \n",
    "    for_plot_df_acc = pd.DataFrame({'Epoch':x,\n",
    "                                    'Accuracy':hist[acc],\n",
    "                                    'Validation Accuracy':hist[val_acc]})\n",
    "    \n",
    "    # Initialize figure\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    # Plot loss\n",
    "    sns.lineplot(x='Epoch',\n",
    "                y='value',\n",
    "                hue='variable',\n",
    "                style='variable',\n",
    "                data=pd.melt(for_plot_df_loss, ['Epoch']),\n",
    "                ax=ax[0],\n",
    "                lw=3)\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    sns.lineplot(x='Epoch',\n",
    "                y='value',\n",
    "                hue='variable',\n",
    "                style='variable',\n",
    "                data=pd.melt(for_plot_df_acc, ['Epoch']),\n",
    "                ax = ax[1],\n",
    "                lw= 3)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "learning_curve(history.history,v1=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Validation Curves (both in Loss and Accuracy) do not divert far from the training curve, we can be confident that there is no overfitting. The value of the accuracy also shows that we are not underfitting in the training and validation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Unseen Data - Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our Vanilla Neural Network, we have attained 82.90% accuracy on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
